<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Mingyu Kim - CV</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic|Open+Sans:300,400,500,700|Waiting+for+the+Sunrise|Shadows+Into+Light" rel="stylesheet" type="text/css">
</head>

<body>
    <div class="wrapper clearfix">
        <div class="left">
            <div class="name-hero">
                <div class="me-img"></div>
                <div class="name-text">
                    <h1>Mingyu <em>Kim</em></h1>
                    <!-- <p>10 Iroquois St Boston, MA 02120</p> -->
                    <p>k012123600@gmail.com</p>
                    <p>20-02-2001</p>
                    <div class="action-buttons">
                        <ul class="skill-set">
                            <a class="btn-download" href="./Mingyu_Kim_s_CVㅇ.pdf" download>
                                <img src="./imgs/cv_down.png" alt="Download CV">
                            </a>
                            <a class="btn-github" href="https://github.com/kMinsAlgorithm" target="_blank" rel="noopener">
                                <img src="./imgs/github_icon.png" alt="GitHub Profile">
                            </a>

                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="right">
            <div class="inner">

                <section>
                    <h1>Welcome</h1>
                    <p>Thank you for visiting my profile page. </p>
                    <p>
                        I am a M.S. candidate in Information and Communication Engineering at Myongji University (MJU), advised by Prof.
                        <a href="https://ds.mju.ac.kr/professor.html" target="_blank" rel="noopener">
                        Jaehee Jung
                      </a>.
                    </p>
                    <p>My master's research focused on enhancing dynamic obstacle avoidance performance in mobile robot navigation by leveraging deep learning models to process semantic information and generate human-like trajectories based on imitation
                        learning. The key research directions I plan to pursue include:</p>


                </section>
                <div class="robot-grid">
                    <a class="robot-item" href="robot1.html">
                        <img src="./imgs/swerve_drive_robot.jpg" alt="Swerve-Drive Robot" class="robot-img explorer">
                        <h3>Swerve-Drive Robot</h3>
                    </a>
                    <a class="robot-item" href="robot2.html">
                        <img src="./imgs/omni_wheel_robot.jpg" alt="Omni-Wheel-Based Robot" class="robot-img delivery">
                        <h3>Omni-Wheel-Based Robot</h3>
                    </a>
                    <a class="robot-item" href="robot3.html">
                        <img src="robot3.png" alt="Survey Bot" class="robot-img survey">
                        <h3>Survey Bot</h3>
                    </a>
                </div>


                <section id="publications">
                    <h1>Publications</h1>
                    <p><em>(* denotes equal contribution)</em></p>

                    <div class="pub-item">
                        <img src="pub1.png" alt="AntMaze result">
                        <div class="pub-content">
                            <h2>Title</h2>
                            <p class="authors">
                                <span class="author-primary">Mingyu Kim*</span>, Chanyeong Heo*, Jaehee Jung
                            </p>
                            <p class="venue">IEEE Access(Under review), 2025</p>
                            <p class="links">
                                <!-- <a href="#">paper</a> / -->
                                <!-- <a href="#">code</a> / -->
                                <!-- <a href="#">website</a> / -->
                                <!-- <a href="#">twitter</a> -->
                            </p>
                            <p class="desc">
                                We introduce a novel model-based offline RL method, Lower Expectile Q-learning (LEQ), which enhances long-horizon task performance by mitigating the high bias in model-based value estimation via expectile regression of λ-returns. Our empirical results
                                show that LEQ significantly outperforms previous model-based offline RL methods on long-horizon tasks, such as the D4RL AntMaze tasks, matching or surpassing the performance of model-free approaches.
                            </p>
                        </div>
                    </div>

                    <div class="pub-item">
                        <img src="pub2.png" alt="Maze exploration">
                        <div class="pub-content">
                            <h2>Title</h2>
                            <p class="authors">
                                <span class="author-primary">Mingyu Kim</span>, Dae Hyun Yum, Jaehee Jung
                            </p>
                            <p class="venue">IEEE Access(Under review), 2025</p>
                            <p class="links">
                                <!-- <a href="#">paper</a> /
                                <a href="#">code</a> /
                                <a href="#">website</a> /
                                <a href="#">twitter</a> -->
                            </p>
                            <p class="desc">
                                We propose a novel unsupervised goal-conditioned RL method, TLDR, which leverages Temporal Distance-aware Representations. Our approach selects faraway goals to initiate exploration and compute intrinsic exploration rewards and goal-reaching rewards.
                                Our experimental results in robotic locomotion and manipulation environments demonstrate that our method significantly outperforms previous unsupervised GCRL methods in achieving a wide variety of states.
                            </p>
                        </div>
                    </div>

                    <div class="pub-item">
                        <img src="pub2.png" alt="Maze exploration">
                        <div class="pub-content">
                            <h2>Title</h2>
                            <p class="authors">
                                Chanyeong Heo, <span class="author-primary">Mingyu Kim</span>, Jaehee Jung
                            </p>
                            <p class="venue">IEEE Access(Under review), 2025</p>
                            <p class="links">
                                <!-- <a href="#">paper</a> /
                              <a href="#">code</a> /
                              <a href="#">website</a> /
                              <a href="#">twitter</a> -->
                            </p>
                            <p class="desc">
                                We propose a novel unsupervised goal-conditioned RL method, TLDR, which leverages Temporal Distance-aware Representations. Our approach selects faraway goals to initiate exploration and compute intrinsic exploration rewards and goal-reaching rewards.
                                Our experimental results in robotic locomotion and manipulation environments demonstrate that our method significantly outperforms previous unsupervised GCRL methods in achieving a wide variety of states.
                            </p>
                        </div>
                    </div>
                </section>


                <section>
                    <h1>Technical Skills</h1>
                    <ul class="skill-set">
                        <li>Mobile Development</li>
                        <li>Xamarin</li>
                        <li>CSS3</li>
                        <li>Adobe Photoshop</li>
                        <li>HTML5</li>
                        <li>JQUERY</li>
                        <li>UI Design</li>
                        <li>Company Branding</li>
                        <li>Responsive Web Design</li>
                    </ul>
                </section>


                <section>
                    <h1>References</h1>
                    <p>William Grand | <em>Grand Interactive, llc. | CEO</em></p>
                    <p>(617) 448-0910 | wgrand@grandinteractive.com</p>
                    <p>Eric Chauvin | <em>PadMatcher Inc. | CEO</em></p>
                    <p>(617) 448-0910 | eric@padmatcher.com</p>
                    <p>Chris Heller | <em>Penrose Realty LLC. | Broker</em></p>
                    <p>(617) 794-4554 | chris@penroserealty.com</p>
                </section>


                <section>
                    <h1>Employment</h1>
                    <p>Winter 2015 - Present <em>Grand Interactive, llc. | Mobile App Developer</em></p>
                    <p>Raised $78,000 in early stage funding, created initial design concepts, and oversaw initial development. Currently oversee and maintain all front end code and server functionality.</p>
                    <p>Spring 2012 - Winter 2015 | <em>PadMatcher Inc. | CTO, Co-Founder</em></p>
                    <p>Raised $78,000 in early stage funding, created initial design concepts, and oversaw initial development. Oversaw and maintained all front end code and server functionality.</p>
                    <p>Fall 2011 - Fall 2013 | <em>Penrose Realty llc. | Designer & Assistant</em></p>
                    <p>Responsible for all technical areas. Maintain servers, computers, and provide in office technical support. Rebranded company from ground up including a fully responsive website. </p>
                </section>



            </div>
        </div>
    </div>
</body>

</html>